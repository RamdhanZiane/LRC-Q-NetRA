<!doctype html>
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />

    <title>train2 API documentation</title>
    <meta name="description" content="" />

  <link href='http://fonts.googleapis.com/css?family=Source+Sans+Pro:400,300' rel='stylesheet' type='text/css'>
  
  <style type="text/css">
  
* {
  box-sizing: border-box;
}
/*! normalize.css v1.1.1 | MIT License | git.io/normalize */

/* ==========================================================================
   HTML5 display definitions
   ========================================================================== */

/**
 * Correct `block` display not defined in IE 6/7/8/9 and Firefox 3.
 */

article,
aside,
details,
figcaption,
figure,
footer,
header,
hgroup,
main,
nav,
section,
summary {
    display: block;
}

/**
 * Correct `inline-block` display not defined in IE 6/7/8/9 and Firefox 3.
 */

audio,
canvas,
video {
    display: inline-block;
    *display: inline;
    *zoom: 1;
}

/**
 * Prevent modern browsers from displaying `audio` without controls.
 * Remove excess height in iOS 5 devices.
 */

audio:not([controls]) {
    display: none;
    height: 0;
}

/**
 * Address styling not present in IE 7/8/9, Firefox 3, and Safari 4.
 * Known issue: no IE 6 support.
 */

[hidden] {
    display: none;
}

/* ==========================================================================
   Base
   ========================================================================== */

/**
 * 1. Prevent system color scheme's background color being used in Firefox, IE,
 *    and Opera.
 * 2. Prevent system color scheme's text color being used in Firefox, IE, and
 *    Opera.
 * 3. Correct text resizing oddly in IE 6/7 when body `font-size` is set using
 *    `em` units.
 * 4. Prevent iOS text size adjust after orientation change, without disabling
 *    user zoom.
 */

html {
    background: #fff; /* 1 */
    color: #000; /* 2 */
    font-size: 100%; /* 3 */
    -webkit-text-size-adjust: 100%; /* 4 */
    -ms-text-size-adjust: 100%; /* 4 */
}

/**
 * Address `font-family` inconsistency between `textarea` and other form
 * elements.
 */

html,
button,
input,
select,
textarea {
    font-family: sans-serif;
}

/**
 * Address margins handled incorrectly in IE 6/7.
 */

body {
    margin: 0;
}

/* ==========================================================================
   Links
   ========================================================================== */

/**
 * Address `outline` inconsistency between Chrome and other browsers.
 */

a:focus {
    outline: thin dotted;
}

/**
 * Improve readability when focused and also mouse hovered in all browsers.
 */

a:active,
a:hover {
    outline: 0;
}

/* ==========================================================================
   Typography
   ========================================================================== */

/**
 * Address font sizes and margins set differently in IE 6/7.
 * Address font sizes within `section` and `article` in Firefox 4+, Safari 5,
 * and Chrome.
 */

h1 {
    font-size: 2em;
    margin: 0.67em 0;
}

h2 {
    font-size: 1.5em;
    margin: 0.83em 0;
}

h3 {
    font-size: 1.17em;
    margin: 1em 0;
}

h4 {
    font-size: 1em;
    margin: 1.33em 0;
}

h5 {
    font-size: 0.83em;
    margin: 1.67em 0;
}

h6 {
    font-size: 0.67em;
    margin: 2.33em 0;
}

/**
 * Address styling not present in IE 7/8/9, Safari 5, and Chrome.
 */

abbr[title] {
    border-bottom: 1px dotted;
}

/**
 * Address style set to `bolder` in Firefox 3+, Safari 4/5, and Chrome.
 */

b,
strong {
    font-weight: bold;
}

blockquote {
    margin: 1em 40px;
}

/**
 * Address styling not present in Safari 5 and Chrome.
 */

dfn {
    font-style: italic;
}

/**
 * Address differences between Firefox and other browsers.
 * Known issue: no IE 6/7 normalization.
 */

hr {
    -moz-box-sizing: content-box;
    box-sizing: content-box;
    height: 0;
}

/**
 * Address styling not present in IE 6/7/8/9.
 */

mark {
    background: #ff0;
    color: #000;
}

/**
 * Address margins set differently in IE 6/7.
 */

p,
pre {
    margin: 1em 0;
}

/**
 * Correct font family set oddly in IE 6, Safari 4/5, and Chrome.
 */

code,
kbd,
pre,
samp {
    font-family: monospace, serif;
    _font-family: 'courier new', monospace;
    font-size: 1em;
}

/**
 * Improve readability of pre-formatted text in all browsers.
 */

pre {
    white-space: pre;
    white-space: pre-wrap;
    word-wrap: break-word;
}

/**
 * Address CSS quotes not supported in IE 6/7.
 */

q {
    quotes: none;
}

/**
 * Address `quotes` property not supported in Safari 4.
 */

q:before,
q:after {
    content: '';
    content: none;
}

/**
 * Address inconsistent and variable font size in all browsers.
 */

small {
    font-size: 80%;
}

/**
 * Prevent `sub` and `sup` affecting `line-height` in all browsers.
 */

sub,
sup {
    font-size: 75%;
    line-height: 0;
    position: relative;
    vertical-align: baseline;
}

sup {
    top: -0.5em;
}

sub {
    bottom: -0.25em;
}

/* ==========================================================================
   Lists
   ========================================================================== */

/**
 * Address margins set differently in IE 6/7.
 */

dl,
menu,
ol,
ul {
    margin: 1em 0;
}

dd {
    margin: 0 0 0 40px;
}

/**
 * Address paddings set differently in IE 6/7.
 */

menu,
ol,
ul {
    padding: 0 0 0 40px;
}

/**
 * Correct list images handled incorrectly in IE 7.
 */

nav ul,
nav ol {
    list-style: none;
    list-style-image: none;
}

/* ==========================================================================
   Embedded content
   ========================================================================== */

/**
 * 1. Remove border when inside `a` element in IE 6/7/8/9 and Firefox 3.
 * 2. Improve image quality when scaled in IE 7.
 */

img {
    border: 0; /* 1 */
    -ms-interpolation-mode: bicubic; /* 2 */
}

/**
 * Correct overflow displayed oddly in IE 9.
 */

svg:not(:root) {
    overflow: hidden;
}

/* ==========================================================================
   Figures
   ========================================================================== */

/**
 * Address margin not present in IE 6/7/8/9, Safari 5, and Opera 11.
 */

figure {
    margin: 0;
}

/* ==========================================================================
   Forms
   ========================================================================== */

/**
 * Correct margin displayed oddly in IE 6/7.
 */

form {
    margin: 0;
}

/**
 * Define consistent border, margin, and padding.
 */

fieldset {
    border: 1px solid #c0c0c0;
    margin: 0 2px;
    padding: 0.35em 0.625em 0.75em;
}

/**
 * 1. Correct color not being inherited in IE 6/7/8/9.
 * 2. Correct text not wrapping in Firefox 3.
 * 3. Correct alignment displayed oddly in IE 6/7.
 */

legend {
    border: 0; /* 1 */
    padding: 0;
    white-space: normal; /* 2 */
    *margin-left: -7px; /* 3 */
}

/**
 * 1. Correct font size not being inherited in all browsers.
 * 2. Address margins set differently in IE 6/7, Firefox 3+, Safari 5,
 *    and Chrome.
 * 3. Improve appearance and consistency in all browsers.
 */

button,
input,
select,
textarea {
    font-size: 100%; /* 1 */
    margin: 0; /* 2 */
    vertical-align: baseline; /* 3 */
    *vertical-align: middle; /* 3 */
}

/**
 * Address Firefox 3+ setting `line-height` on `input` using `!important` in
 * the UA stylesheet.
 */

button,
input {
    line-height: normal;
}

/**
 * Address inconsistent `text-transform` inheritance for `button` and `select`.
 * All other form control elements do not inherit `text-transform` values.
 * Correct `button` style inheritance in Chrome, Safari 5+, and IE 6+.
 * Correct `select` style inheritance in Firefox 4+ and Opera.
 */

button,
select {
    text-transform: none;
}

/**
 * 1. Avoid the WebKit bug in Android 4.0.* where (2) destroys native `audio`
 *    and `video` controls.
 * 2. Correct inability to style clickable `input` types in iOS.
 * 3. Improve usability and consistency of cursor style between image-type
 *    `input` and others.
 * 4. Remove inner spacing in IE 7 without affecting normal text inputs.
 *    Known issue: inner spacing remains in IE 6.
 */

button,
html input[type="button"], /* 1 */
input[type="reset"],
input[type="submit"] {
    -webkit-appearance: button; /* 2 */
    cursor: pointer; /* 3 */
    *overflow: visible;  /* 4 */
}

/**
 * Re-set default cursor for disabled elements.
 */

button[disabled],
html input[disabled] {
    cursor: default;
}

/**
 * 1. Address box sizing set to content-box in IE 8/9.
 * 2. Remove excess padding in IE 8/9.
 * 3. Remove excess padding in IE 7.
 *    Known issue: excess padding remains in IE 6.
 */

input[type="checkbox"],
input[type="radio"] {
    box-sizing: border-box; /* 1 */
    padding: 0; /* 2 */
    *height: 13px; /* 3 */
    *width: 13px; /* 3 */
}

/**
 * 1. Address `appearance` set to `searchfield` in Safari 5 and Chrome.
 * 2. Address `box-sizing` set to `border-box` in Safari 5 and Chrome
 *    (include `-moz` to future-proof).
 */

input[type="search"] {
    -webkit-appearance: textfield; /* 1 */
    -moz-box-sizing: content-box;
    -webkit-box-sizing: content-box; /* 2 */
    box-sizing: content-box;
}

/**
 * Remove inner padding and search cancel button in Safari 5 and Chrome
 * on OS X.
 */

input[type="search"]::-webkit-search-cancel-button,
input[type="search"]::-webkit-search-decoration {
    -webkit-appearance: none;
}

/**
 * Remove inner padding and border in Firefox 3+.
 */

button::-moz-focus-inner,
input::-moz-focus-inner {
    border: 0;
    padding: 0;
}

/**
 * 1. Remove default vertical scrollbar in IE 6/7/8/9.
 * 2. Improve readability and alignment in all browsers.
 */

textarea {
    overflow: auto; /* 1 */
    vertical-align: top; /* 2 */
}

/* ==========================================================================
   Tables
   ========================================================================== */

/**
 * Remove most spacing between table cells.
 */

table {
    border-collapse: collapse;
    border-spacing: 0;
}

  </style>

  <style type="text/css">
  
  html, body {
    margin: 0;
    padding: 0;
    min-height: 100%;
  }
  body {
    background: #fff;
    font-family: "Source Sans Pro", "Helvetica Neueue", Helvetica, sans;
    font-weight: 300;
    font-size: 16px;
    line-height: 1.6em;
  }
  #content {
    width: 70%;
    max-width: 850px;
    float: left;
    padding: 30px 60px;
    border-left: 1px solid #ddd;
  }
  #sidebar {
    width: 25%;
    float: left;
    padding: 30px;
    overflow: hidden;
  }
  #nav {
    font-size: 130%;
    margin: 0 0 15px 0;
  }

  #top {
    display: block;
    position: fixed;
    bottom: 5px;
    left: 5px;
    font-size: .85em;
    text-transform: uppercase;
  }

  #footer {
    font-size: .75em;
    padding: 5px 30px;
    border-top: 1px solid #ddd;
    text-align: right;
  }
    #footer p {
      margin: 0 0 0 30px;
      display: inline-block;
    }

  h1, h2, h3, h4, h5 {
    font-weight: 300;
  }
  h1 {
    font-size: 2.5em;
    line-height: 1.1em;
    margin: 0 0 .50em 0;
  }

  h2 {
    font-size: 1.75em;
    margin: 1em 0 .50em 0;
  }

  h3 {
    margin: 25px 0 10px 0;
  }

  h4 {
    margin: 0;
    font-size: 105%;
  }

  a {
    color: #058;
    text-decoration: none;
    transition: color .3s ease-in-out;
  }

  a:hover {
    color: #e08524;
    transition: color .3s ease-in-out;
  }

  pre, code, .mono, .name {
    font-family: "Ubuntu Mono", "Cousine", "DejaVu Sans Mono", monospace;
  }

  .title .name {
    font-weight: bold;
  }
  .section-title {
    margin-top: 2em;
  }
  .ident {
    color: #900;
  }

  code {
    background: #f9f9f9;
  } 

  pre {
    background: #fefefe;
    border: 1px solid #ddd;
    box-shadow: 2px 2px 0 #f3f3f3;
    margin: 0 30px;
    padding: 15px 30px;
  }

  .codehilite {
    margin: 0 30px 10px 30px;
  }

    .codehilite pre {
      margin: 0;
    }
    .codehilite .err { background: #ff3300; color: #fff !important; } 

  table#module-list {
    font-size: 110%;
  }

    table#module-list tr td:first-child {
      padding-right: 10px;
      white-space: nowrap;
    }

    table#module-list td {
      vertical-align: top;
      padding-bottom: 8px;
    }

      table#module-list td p {
        margin: 0 0 7px 0;
      }

  .def {
    display: table;
  }

    .def p {
      display: table-cell;
      vertical-align: top;
      text-align: left;
    }

    .def p:first-child {
      white-space: nowrap;
    }

    .def p:last-child {
      width: 100%;
    }


  #index {
    list-style-type: none;
    margin: 0;
    padding: 0;
  }
    ul#index .class_name {
      /* font-size: 110%; */
      font-weight: bold;
    }
    #index ul {
      margin: 0;
    }

  .item {
    margin: 0 0 15px 0;
  }

    .item .class {
      margin: 0 0 25px 30px;
    }

      .item .class ul.class_list {
        margin: 0 0 20px 0;
      }

    .item .name {
      background: #fafafa;
      margin: 0;
      font-weight: bold;
      padding: 5px 10px;
      border-radius: 3px;
      display: inline-block;
      min-width: 40%;
    }
      .item .name:hover {
        background: #f6f6f6;
      }

    .item .empty_desc {
      margin: 0 0 5px 0;
      padding: 0;
    }

    .item .inheritance {
      margin: 3px 0 0 30px;
    }

    .item .inherited {
      color: #666;
    }

    .item .desc {
      padding: 0 8px;
      margin: 0;
    }

      .item .desc p {
        margin: 0 0 10px 0;
      }

    .source_cont {
      margin: 0;
      padding: 0;
    }

    .source_link a {
      background: #ffc300;
      font-weight: 400;
      font-size: .75em;
      text-transform: uppercase;
      color: #fff;
      text-shadow: 1px 1px 0 #f4b700;
      
      padding: 3px 8px;
      border-radius: 2px;
      transition: background .3s ease-in-out;
    }
      .source_link a:hover {
        background: #FF7200;
        text-shadow: none;
        transition: background .3s ease-in-out;
      }

    .source {
      display: none;
      max-height: 600px;
      overflow-y: scroll;
      margin-bottom: 15px;
    }

      .source .codehilite {
        margin: 0;
      }

  .desc h1, .desc h2, .desc h3 {
    font-size: 100% !important;
  }
  .clear {
    clear: both;
  }

  @media all and (max-width: 950px) {
    #sidebar {
      width: 35%;
    }
    #content {
      width: 65%;
    }
  }
  @media all and (max-width: 650px) {
    #top {
      display: none;
    }
    #sidebar {
      float: none;
      width: auto;
    }
    #content {
      float: none;
      width: auto;
      padding: 30px;
    }

    #index ul {
      padding: 0;
      margin-bottom: 15px;
    }
    #index ul li {
      display: inline-block;
      margin-right: 30px;
    }
    #footer {
      text-align: left;
    }
    #footer p {
      display: block;
      margin: inherit;
    }
  }

  /*****************************/

  </style>

  <style type="text/css">
  .codehilite .hll { background-color: #ffffcc }
.codehilite  { background: #f8f8f8; }
.codehilite .c { color: #408080; font-style: italic } /* Comment */
.codehilite .err { border: 1px solid #FF0000 } /* Error */
.codehilite .k { color: #008000; font-weight: bold } /* Keyword */
.codehilite .o { color: #666666 } /* Operator */
.codehilite .ch { color: #408080; font-style: italic } /* Comment.Hashbang */
.codehilite .cm { color: #408080; font-style: italic } /* Comment.Multiline */
.codehilite .cp { color: #BC7A00 } /* Comment.Preproc */
.codehilite .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */
.codehilite .c1 { color: #408080; font-style: italic } /* Comment.Single */
.codehilite .cs { color: #408080; font-style: italic } /* Comment.Special */
.codehilite .gd { color: #A00000 } /* Generic.Deleted */
.codehilite .ge { font-style: italic } /* Generic.Emph */
.codehilite .gr { color: #FF0000 } /* Generic.Error */
.codehilite .gh { color: #000080; font-weight: bold } /* Generic.Heading */
.codehilite .gi { color: #00A000 } /* Generic.Inserted */
.codehilite .go { color: #888888 } /* Generic.Output */
.codehilite .gp { color: #000080; font-weight: bold } /* Generic.Prompt */
.codehilite .gs { font-weight: bold } /* Generic.Strong */
.codehilite .gu { color: #800080; font-weight: bold } /* Generic.Subheading */
.codehilite .gt { color: #0044DD } /* Generic.Traceback */
.codehilite .kc { color: #008000; font-weight: bold } /* Keyword.Constant */
.codehilite .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */
.codehilite .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */
.codehilite .kp { color: #008000 } /* Keyword.Pseudo */
.codehilite .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */
.codehilite .kt { color: #B00040 } /* Keyword.Type */
.codehilite .m { color: #666666 } /* Literal.Number */
.codehilite .s { color: #BA2121 } /* Literal.String */
.codehilite .na { color: #7D9029 } /* Name.Attribute */
.codehilite .nb { color: #008000 } /* Name.Builtin */
.codehilite .nc { color: #0000FF; font-weight: bold } /* Name.Class */
.codehilite .no { color: #880000 } /* Name.Constant */
.codehilite .nd { color: #AA22FF } /* Name.Decorator */
.codehilite .ni { color: #999999; font-weight: bold } /* Name.Entity */
.codehilite .ne { color: #D2413A; font-weight: bold } /* Name.Exception */
.codehilite .nf { color: #0000FF } /* Name.Function */
.codehilite .nl { color: #A0A000 } /* Name.Label */
.codehilite .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */
.codehilite .nt { color: #008000; font-weight: bold } /* Name.Tag */
.codehilite .nv { color: #19177C } /* Name.Variable */
.codehilite .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */
.codehilite .w { color: #bbbbbb } /* Text.Whitespace */
.codehilite .mb { color: #666666 } /* Literal.Number.Bin */
.codehilite .mf { color: #666666 } /* Literal.Number.Float */
.codehilite .mh { color: #666666 } /* Literal.Number.Hex */
.codehilite .mi { color: #666666 } /* Literal.Number.Integer */
.codehilite .mo { color: #666666 } /* Literal.Number.Oct */
.codehilite .sa { color: #BA2121 } /* Literal.String.Affix */
.codehilite .sb { color: #BA2121 } /* Literal.String.Backtick */
.codehilite .sc { color: #BA2121 } /* Literal.String.Char */
.codehilite .dl { color: #BA2121 } /* Literal.String.Delimiter */
.codehilite .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */
.codehilite .s2 { color: #BA2121 } /* Literal.String.Double */
.codehilite .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */
.codehilite .sh { color: #BA2121 } /* Literal.String.Heredoc */
.codehilite .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */
.codehilite .sx { color: #008000 } /* Literal.String.Other */
.codehilite .sr { color: #BB6688 } /* Literal.String.Regex */
.codehilite .s1 { color: #BA2121 } /* Literal.String.Single */
.codehilite .ss { color: #19177C } /* Literal.String.Symbol */
.codehilite .bp { color: #008000 } /* Name.Builtin.Pseudo */
.codehilite .fm { color: #0000FF } /* Name.Function.Magic */
.codehilite .vc { color: #19177C } /* Name.Variable.Class */
.codehilite .vg { color: #19177C } /* Name.Variable.Global */
.codehilite .vi { color: #19177C } /* Name.Variable.Instance */
.codehilite .vm { color: #19177C } /* Name.Variable.Magic */
.codehilite .il { color: #666666 } /* Literal.Number.Integer.Long */
  </style>

  <style type="text/css">
  
/* ==========================================================================
   EXAMPLE Media Queries for Responsive Design.
   These examples override the primary ('mobile first') styles.
   Modify as content requires.
   ========================================================================== */

@media only screen and (min-width: 35em) {
    /* Style adjustments for viewports that meet the condition */
}

@media print,
       (-o-min-device-pixel-ratio: 5/4),
       (-webkit-min-device-pixel-ratio: 1.25),
       (min-resolution: 120dpi) {
    /* Style adjustments for high resolution devices */
}

/* ==========================================================================
   Print styles.
   Inlined to avoid required HTTP connection: h5bp.com/r
   ========================================================================== */

@media print {
    * {
        background: transparent !important;
        color: #000 !important; /* Black prints faster: h5bp.com/s */
        box-shadow: none !important;
        text-shadow: none !important;
    }

    a,
    a:visited {
        text-decoration: underline;
    }

    a[href]:after {
        content: " (" attr(href) ")";
    }

    abbr[title]:after {
        content: " (" attr(title) ")";
    }

    /*
     * Don't show links for images, or javascript/internal links
     */

    .ir a:after,
    a[href^="javascript:"]:after,
    a[href^="#"]:after {
        content: "";
    }

    pre,
    blockquote {
        border: 1px solid #999;
        page-break-inside: avoid;
    }

    thead {
        display: table-header-group; /* h5bp.com/t */
    }

    tr,
    img {
        page-break-inside: avoid;
    }

    img {
        max-width: 100% !important;
    }

    @page {
        margin: 0.5cm;
    }

    p,
    h2,
    h3 {
        orphans: 3;
        widows: 3;
    }

    h2,
    h3 {
        page-break-after: avoid;
    }
}

  </style>

  <script type="text/javascript">
  function toggle(id, $link) {
    $node = document.getElementById(id);
    if (!$node)
    return;
    if (!$node.style.display || $node.style.display == 'none') {
    $node.style.display = 'block';
    $link.innerHTML = 'Hide source &nequiv;';
    } else {
    $node.style.display = 'none';
    $link.innerHTML = 'Show source &equiv;';
    }
  }
  </script>
</head>
<body>
<a href="#" id="top">Top</a>

<div id="container">
    
  
  <div id="sidebar">
    <h1>Index</h1>
    <ul id="index">
    <li class="set"><h3><a href="#header-variables">Module variables</a></h3>
      
  <ul>
    <li class="mono"><a href="#train2.A">A</a></li>
    <li class="mono"><a href="#train2.A_nx">A_nx</a></li>
    <li class="mono"><a href="#train2.EMBED_SEGMENT">EMBED_SEGMENT</a></li>
    <li class="mono"><a href="#train2.L">L</a></li>
    <li class="mono"><a href="#train2.args_arch_d">args_arch_d</a></li>
    <li class="mono"><a href="#train2.args_arch_g">args_arch_g</a></li>
    <li class="mono"><a href="#train2.args_batch_size">args_batch_size</a></li>
    <li class="mono"><a href="#train2.args_beta1">args_beta1</a></li>
    <li class="mono"><a href="#train2.args_clip">args_clip</a></li>
    <li class="mono"><a href="#train2.args_cuda">args_cuda</a></li>
    <li class="mono"><a href="#train2.args_data_path">args_data_path</a></li>
    <li class="mono"><a href="#train2.args_dropout">args_dropout</a></li>
    <li class="mono"><a href="#train2.args_emsize">args_emsize</a></li>
    <li class="mono"><a href="#train2.args_enc_grad_norm">args_enc_grad_norm</a></li>
    <li class="mono"><a href="#train2.args_epochs">args_epochs</a></li>
    <li class="mono"><a href="#train2.args_gan_clamp">args_gan_clamp</a></li>
    <li class="mono"><a href="#train2.args_gan_toenc">args_gan_toenc</a></li>
    <li class="mono"><a href="#train2.args_hidden_init">args_hidden_init</a></li>
    <li class="mono"><a href="#train2.args_log_interval">args_log_interval</a></li>
    <li class="mono"><a href="#train2.args_lr_ae">args_lr_ae</a></li>
    <li class="mono"><a href="#train2.args_lr_gan_d">args_lr_gan_d</a></li>
    <li class="mono"><a href="#train2.args_lr_gan_g">args_lr_gan_g</a></li>
    <li class="mono"><a href="#train2.args_maxlen">args_maxlen</a></li>
    <li class="mono"><a href="#train2.args_min_epochs">args_min_epochs</a></li>
    <li class="mono"><a href="#train2.args_nhidden">args_nhidden</a></li>
    <li class="mono"><a href="#train2.args_niters_ae">args_niters_ae</a></li>
    <li class="mono"><a href="#train2.args_niters_gan_d">args_niters_gan_d</a></li>
    <li class="mono"><a href="#train2.args_niters_gan_g">args_niters_gan_g</a></li>
    <li class="mono"><a href="#train2.args_niters_gan_schedule">args_niters_gan_schedule</a></li>
    <li class="mono"><a href="#train2.args_nlayers">args_nlayers</a></li>
    <li class="mono"><a href="#train2.args_no_earlystopping">args_no_earlystopping</a></li>
    <li class="mono"><a href="#train2.args_noise_anneal">args_noise_anneal</a></li>
    <li class="mono"><a href="#train2.args_noise_radius">args_noise_radius</a></li>
    <li class="mono"><a href="#train2.args_ntokens">args_ntokens</a></li>
    <li class="mono"><a href="#train2.args_numWalks_per_node">args_numWalks_per_node</a></li>
    <li class="mono"><a href="#train2.args_outf">args_outf</a></li>
    <li class="mono"><a href="#train2.args_sample">args_sample</a></li>
    <li class="mono"><a href="#train2.args_seed">args_seed</a></li>
    <li class="mono"><a href="#train2.args_temp">args_temp</a></li>
    <li class="mono"><a href="#train2.args_walk_length">args_walk_length</a></li>
    <li class="mono"><a href="#train2.args_z_size">args_z_size</a></li>
    <li class="mono"><a href="#train2.autoencoder">autoencoder</a></li>
    <li class="mono"><a href="#train2.corpus">corpus</a></li>
    <li class="mono"><a href="#train2.criterion_ce">criterion_ce</a></li>
    <li class="mono"><a href="#train2.embedding_path">embedding_path</a></li>
    <li class="mono"><a href="#train2.epoch">epoch</a></li>
    <li class="mono"><a href="#train2.epoch_start_time">epoch_start_time</a></li>
    <li class="mono"><a href="#train2.f">f</a></li>
    <li class="mono"><a href="#train2.fixed_noise">fixed_noise</a></li>
    <li class="mono"><a href="#train2.gan_disc">gan_disc</a></li>
    <li class="mono"><a href="#train2.gan_gen">gan_gen</a></li>
    <li class="mono"><a href="#train2.gan_schedule">gan_schedule</a></li>
    <li class="mono"><a href="#train2.i">i</a></li>
    <li class="mono"><a href="#train2.k">k</a></li>
    <li class="mono"><a href="#train2.membership_path">membership_path</a></li>
    <li class="mono"><a href="#train2.mone">mone</a></li>
    <li class="mono"><a href="#train2.niter">niter</a></li>
    <li class="mono"><a href="#train2.niter_gan">niter_gan</a></li>
    <li class="mono"><a href="#train2.niter_global">niter_global</a></li>
    <li class="mono"><a href="#train2.ntokens">ntokens</a></li>
    <li class="mono"><a href="#train2.one">one</a></li>
    <li class="mono"><a href="#train2.optimizer_ae">optimizer_ae</a></li>
    <li class="mono"><a href="#train2.optimizer_gan_d">optimizer_gan_d</a></li>
    <li class="mono"><a href="#train2.optimizer_gan_g">optimizer_gan_g</a></li>
    <li class="mono"><a href="#train2.start_time">start_time</a></li>
    <li class="mono"><a href="#train2.train_data">train_data</a></li>
    <li class="mono"><a href="#train2.walks">walks</a></li>
    <li class="mono"><a href="#train2.x">x</a></li>
  </ul>

    </li>

    <li class="set"><h3><a href="#header-functions">Functions</a></h3>
      
  <ul>
    <li class="mono"><a href="#train2.embed_afterLSTM">embed_afterLSTM</a></li>
    <li class="mono"><a href="#train2.embed_dic_for_loss">embed_dic_for_loss</a></li>
    <li class="mono"><a href="#train2.grad_hook">grad_hook</a></li>
    <li class="mono"><a href="#train2.save_model">save_model</a></li>
    <li class="mono"><a href="#train2.train_ae">train_ae</a></li>
    <li class="mono"><a href="#train2.train_gan_d">train_gan_d</a></li>
    <li class="mono"><a href="#train2.train_gan_g">train_gan_g</a></li>
    <li class="mono"><a href="#train2.unique">unique</a></li>
  </ul>

    </li>


    </ul>
  </div>

    <article id="content">
      
  

  


  <header id="section-intro">
  <h1 class="title"><span class="name">train2</span> module</h1>
  
  
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-train2', this);">Show source &equiv;</a></p>
  <div id="source-train2" class="source">
    <div class="codehilite"><pre><span></span><span class="c1"># for parsing input command</span>
<span class="kn">import</span> <span class="nn">argparse</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">random</span>

<span class="c1"># for logging</span>
<span class="kn">import</span> <span class="nn">json</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="kn">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="kn">as</span> <span class="nn">optim</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="kn">as</span> <span class="nn">F</span>
<span class="kn">from</span> <span class="nn">torch.autograd</span> <span class="kn">import</span> <span class="n">Variable</span>

<span class="c1"># import utils for data preparation and algorithmic models</span>
<span class="kn">from</span> <span class="nn">utils</span> <span class="kn">import</span> <span class="n">to_gpu</span><span class="p">,</span> <span class="n">Corpus</span><span class="p">,</span> <span class="n">batchify</span><span class="p">,</span> <span class="n">generate_walks</span>
<span class="kn">from</span> <span class="nn">models</span> <span class="kn">import</span> <span class="n">Seq2Seq</span><span class="p">,</span> <span class="n">MLP_D</span><span class="p">,</span> <span class="n">MLP_G</span>

<span class="c1"># import visualization module</span>
<span class="kn">from</span> <span class="nn">viz_karate</span> <span class="kn">import</span> <span class="n">viz</span>

<span class="n">EMBED_SEGMENT</span> <span class="o">=</span> <span class="mi">4000</span>

<span class="c1"># import networkx package for graph data input parse, need to install networkx by: pip install networkx</span>
<span class="kn">import</span> <span class="nn">networkx</span> <span class="kn">as</span> <span class="nn">nx</span>

<span class="c1"># import scipy package for geting laplacian of graph, need to install networkx by: pip install scipy</span>
<span class="kn">from</span> <span class="nn">scipy.sparse</span> <span class="kn">import</span> <span class="n">csgraph</span>





<span class="n">args_outf</span> <span class="o">=</span> <span class="s1">&#39;example&#39;</span>
<span class="n">args_data_path</span> <span class="o">=</span> <span class="s1">&#39;../data/karate.adjlist&#39;</span>
<span class="n">args_maxlen</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">args_nhidden</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">args_emsize</span> <span class="o">=</span> <span class="mi">30</span>
<span class="n">args_nlayers</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">args_noise_radius</span> <span class="o">=</span><span class="mf">0.2</span>
<span class="n">args_noise_anneal</span> <span class="o">=</span> <span class="mf">0.995</span>
<span class="n">args_hidden_init</span> <span class="o">=</span> <span class="s1">&#39;store_true&#39;</span>
<span class="n">args_arch_g</span> <span class="o">=</span> <span class="s1">&#39;300-300&#39;</span>
<span class="n">args_arch_d</span> <span class="o">=</span> <span class="s1">&#39;300-300&#39;</span>
<span class="n">args_z_size</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">args_temp</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">args_enc_grad_norm</span> <span class="o">=</span><span class="bp">True</span>
<span class="n">args_gan_toenc</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.01</span>
<span class="n">args_dropout</span> <span class="o">=</span><span class="mi">0</span>
<span class="n">args_epochs</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">args_walk_length</span> <span class="o">=</span><span class="mi">20</span>
<span class="n">args_numWalks_per_node</span> <span class="o">=</span><span class="mi">30</span>
<span class="n">args_batch_size</span> <span class="o">=</span><span class="mi">64</span>
<span class="n">args_niters_ae</span> <span class="o">=</span><span class="mi">1</span>
<span class="n">args_niters_gan_d</span> <span class="o">=</span><span class="mi">5</span>
<span class="n">args_niters_gan_g</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">args_niters_gan_schedule</span> <span class="o">=</span><span class="s1">&#39;2-4-6-10-20-30-40&#39;</span>
<span class="n">args_min_epochs</span> <span class="o">=</span> <span class="mi">6</span>
<span class="n">args_no_earlystopping</span> <span class="o">=</span><span class="s1">&#39;store_true&#39;</span>
<span class="n">args_lr_ae</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">args_lr_gan_g</span> <span class="o">=</span> <span class="mf">0.005</span>
<span class="n">args_lr_gan_d</span> <span class="o">=</span><span class="mf">0.001</span>
<span class="n">args_beta1</span> <span class="o">=</span> <span class="mf">0.9</span>
<span class="n">args_clip</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">args_gan_clamp</span> <span class="o">=</span><span class="mf">0.01</span>
<span class="n">args_sample</span> <span class="o">=</span> <span class="s1">&#39;store_true&#39;</span>
<span class="n">args_log_interval</span> <span class="o">=</span><span class="mi">200</span>
<span class="n">args_seed</span> <span class="o">=</span> <span class="mi">1111</span>
<span class="n">args_cuda</span> <span class="o">=</span> <span class="bp">False</span>




<span class="c1"># make output directory if it doesn&#39;t already exist</span>
<span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isdir</span><span class="p">(</span><span class="s1">&#39;./output&#39;</span><span class="p">):</span>
    <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="s1">&#39;./output&#39;</span><span class="p">)</span>
<span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isdir</span><span class="p">(</span><span class="s1">&#39;./output/{}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">args_outf</span><span class="p">)):</span>
    <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="s1">&#39;./output/{}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">args_outf</span><span class="p">))</span>


<span class="c1"># Set the random seed manually for reproducibility.</span>
<span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">args_seed</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">args_seed</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">args_seed</span><span class="p">)</span>
<span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">args_cuda</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="s2">&quot;WARNING: You have a CUDA device, &quot;</span>
              <span class="s2">&quot;so you should probably run with --cuda&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">args_seed</span><span class="p">)</span>


<span class="c1">###############################################################################</span>
<span class="c1"># Load data</span>
<span class="c1">###############################################################################</span>

<span class="k">print</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">getcwd</span><span class="p">())</span>
<span class="c1"># A_nx = nx.read_edgelist(args_data_path,nodetype = int, create_using=nx.DiGraph())  # if the graph is with sparse edge format, each line one edge, then use this</span>
<span class="n">A_nx</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">read_adjlist</span><span class="p">(</span><span class="n">args_data_path</span><span class="p">,</span> <span class="n">nodetype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>                        <span class="c1"># use this for reading adjacent list format graph</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">to_scipy_sparse_matrix</span><span class="p">(</span><span class="n">A_nx</span><span class="p">)</span>                                         <span class="c1"># transfer to sparse matrix format</span>
<span class="n">L</span> <span class="o">=</span> <span class="n">csgraph</span><span class="o">.</span><span class="n">laplacian</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">normed</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>                                      <span class="c1"># use csgraph package to calculate the laplacian</span>
<span class="n">L</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">L</span><span class="o">.</span><span class="n">toarray</span><span class="p">(),</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">L</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">L</span><span class="p">))</span>
<span class="n">L</span> <span class="o">=</span> <span class="n">to_gpu</span><span class="p">(</span><span class="n">args_cuda</span><span class="p">,</span> <span class="n">L</span><span class="p">)</span>





<span class="c1"># generate walk for each node with given walk_length</span>
<span class="n">walks</span> <span class="o">=</span> <span class="n">generate_walks</span><span class="p">(</span><span class="n">A_nx</span><span class="p">,</span> <span class="n">args_numWalks_per_node</span><span class="p">,</span> <span class="n">args_walk_length</span><span class="p">)</span>
<span class="c1"># save randomly generated walks to file ../tmp/train.txt</span>
<span class="n">np</span><span class="o">.</span><span class="n">savetxt</span><span class="p">(</span><span class="s1">&#39;../tmp/train.txt&#39;</span><span class="p">,</span> <span class="n">walks</span><span class="p">,</span> <span class="n">delimiter</span><span class="o">=</span><span class="s2">&quot; &quot;</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="s2">&quot;</span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># create corpus</span>
<span class="n">corpus</span> <span class="o">=</span> <span class="n">Corpus</span><span class="p">(</span><span class="s1">&#39;../tmp/&#39;</span><span class="p">,</span>
                <span class="n">maxlen</span><span class="o">=</span><span class="n">args_maxlen</span><span class="p">)</span>
<span class="c1"># dumping vocabulary</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;./output/{}/vocab.json&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">args_outf</span><span class="p">),</span> <span class="s1">&#39;w&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">json</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">corpus</span><span class="o">.</span><span class="n">dictionary</span><span class="o">.</span><span class="n">word2idx</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span>

<span class="c1"># save arguments to args_json and logs.txt</span>
<span class="n">ntokens</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">corpus</span><span class="o">.</span><span class="n">dictionary</span><span class="o">.</span><span class="n">word2idx</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Vocabulary Size: {}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">ntokens</span><span class="p">))</span>
<span class="n">args_ntokens</span> <span class="o">=</span> <span class="n">ntokens</span>


<span class="c1"># preparing batches for training</span>
<span class="n">train_data</span> <span class="o">=</span> <span class="n">batchify</span><span class="p">(</span><span class="n">corpus</span><span class="o">.</span><span class="n">train</span><span class="p">,</span> <span class="n">args_batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Loaded data!&quot;</span><span class="p">)</span>

<span class="c1">###############################################################################</span>
<span class="c1"># Build the models</span>
<span class="c1">###############################################################################</span>

<span class="n">ntokens</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">corpus</span><span class="o">.</span><span class="n">dictionary</span><span class="o">.</span><span class="n">word2idx</span><span class="p">)</span>
<span class="n">autoencoder</span> <span class="o">=</span> <span class="n">Seq2Seq</span><span class="p">(</span><span class="n">emsize</span><span class="o">=</span><span class="n">args_emsize</span><span class="p">,</span>
                      <span class="n">nhidden</span><span class="o">=</span><span class="n">args_nhidden</span><span class="p">,</span>
                      <span class="n">ntokens</span><span class="o">=</span><span class="n">ntokens</span><span class="p">,</span>
                      <span class="n">nlayers</span><span class="o">=</span><span class="n">args_nlayers</span><span class="p">,</span>
                      <span class="n">noise_radius</span><span class="o">=</span><span class="n">args_noise_radius</span><span class="p">,</span>
                      <span class="n">hidden_init</span><span class="o">=</span><span class="n">args_hidden_init</span><span class="p">,</span>
                      <span class="n">dropout</span><span class="o">=</span><span class="n">args_dropout</span><span class="p">,</span>
                      <span class="n">gpu</span><span class="o">=</span><span class="n">args_cuda</span><span class="p">)</span>

<span class="n">gan_gen</span> <span class="o">=</span> <span class="n">MLP_G</span><span class="p">(</span><span class="n">ninput</span><span class="o">=</span><span class="n">args_z_size</span><span class="p">,</span> <span class="n">noutput</span><span class="o">=</span><span class="n">args_nhidden</span><span class="p">,</span> <span class="n">layers</span><span class="o">=</span><span class="n">args_arch_g</span><span class="p">)</span>
<span class="n">gan_disc</span> <span class="o">=</span> <span class="n">MLP_D</span><span class="p">(</span><span class="n">ninput</span><span class="o">=</span><span class="n">args_nhidden</span><span class="p">,</span> <span class="n">noutput</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">layers</span><span class="o">=</span><span class="n">args_arch_d</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">autoencoder</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">gan_gen</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">gan_disc</span><span class="p">)</span>


<span class="c1">#### optimizing AE, GAN-generator, GAN-discriminator</span>
<span class="c1">## SGD, learning rate should be larger, like 1, Adam&#39;s learning rate should be smaller, like 0.001</span>
<span class="n">optimizer_ae</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">autoencoder</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">args_lr_ae</span><span class="p">)</span>
<span class="n">optimizer_gan_g</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">gan_gen</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span>
                             <span class="n">lr</span><span class="o">=</span><span class="n">args_lr_gan_g</span><span class="p">,</span>
                             <span class="n">betas</span><span class="o">=</span><span class="p">(</span><span class="n">args_beta1</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">))</span>
<span class="n">optimizer_gan_d</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">gan_disc</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span>
                             <span class="n">lr</span><span class="o">=</span><span class="n">args_lr_gan_d</span><span class="p">,</span>
                             <span class="n">betas</span><span class="o">=</span><span class="p">(</span><span class="n">args_beta1</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">))</span>
<span class="c1">#### crossEntropy loss for discriminator</span>
<span class="n">criterion_ce</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>

<span class="k">if</span> <span class="n">args_cuda</span><span class="p">:</span>
    <span class="n">autoencoder</span> <span class="o">=</span> <span class="n">autoencoder</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
    <span class="n">gan_gen</span> <span class="o">=</span> <span class="n">gan_gen</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
    <span class="n">gan_disc</span> <span class="o">=</span> <span class="n">gan_disc</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
    <span class="n">criterion_ce</span> <span class="o">=</span> <span class="n">criterion_ce</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>


<span class="c1">###############################################################################</span>
<span class="c1"># Training code</span>
<span class="c1">###############################################################################</span>


<span class="k">def</span> <span class="nf">save_model</span><span class="p">():</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">&quot;Saving models&quot;</span><span class="p">)</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;./output/{}/autoencoder_model.pt&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">args_outf</span><span class="p">),</span> <span class="s1">&#39;wb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">autoencoder</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">f</span><span class="p">)</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;./output/{}/gan_gen_model.pt&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">args_outf</span><span class="p">),</span> <span class="s1">&#39;wb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">gan_gen</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">f</span><span class="p">)</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;./output/{}/gan_disc_model.pt&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">args_outf</span><span class="p">),</span> <span class="s1">&#39;wb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">gan_disc</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">f</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">embed_afterLSTM</span><span class="p">(</span><span class="n">corpus</span><span class="p">,</span> <span class="n">emsize</span><span class="p">):</span>
    <span class="n">dic</span> <span class="o">=</span> <span class="n">corpus</span><span class="o">.</span><span class="n">dictionary</span><span class="o">.</span><span class="n">word2idx</span><span class="o">.</span><span class="n">values</span><span class="p">()</span>
    <span class="n">dic</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">dic</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">ntokens</span> <span class="o">&lt;=</span> <span class="n">EMBED_SEGMENT</span><span class="p">:</span>
        <span class="n">dic</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">((</span><span class="n">dic</span><span class="p">,</span><span class="n">dic</span><span class="p">))</span>
        <span class="n">dic_to_embed</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">dic</span><span class="p">))</span>
        <span class="n">dic_to_embed</span> <span class="o">=</span> <span class="n">to_gpu</span><span class="p">(</span><span class="n">args_cuda</span><span class="p">,</span> <span class="n">dic_to_embed</span><span class="p">)</span>
        <span class="n">embeded</span> <span class="o">=</span> <span class="n">autoencoder</span><span class="o">.</span><span class="n">embed_after_LSTM</span><span class="p">(</span><span class="n">dic_to_embed</span><span class="p">,</span> <span class="p">[</span><span class="n">ntokens</span><span class="p">,</span><span class="n">ntokens</span><span class="p">])</span>
        <span class="n">dic_vector</span> <span class="o">=</span> <span class="n">embeded</span><span class="p">[:</span><span class="n">ntokens</span><span class="p">]</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">dic_i</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array_split</span><span class="p">(</span><span class="n">dic</span><span class="p">,</span> <span class="n">ntokens</span><span class="o">/</span><span class="n">EMBED_SEGMENT</span><span class="p">)</span>
        <span class="n">dic_1</span> <span class="o">=</span> <span class="n">dic_i</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">n_dic_1</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dic_1</span><span class="p">)</span>
        <span class="n">dic_1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">((</span><span class="n">dic_1</span><span class="p">,</span><span class="n">dic_1</span><span class="p">))</span>
        <span class="n">dic_to_embed</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">dic_1</span><span class="p">))</span>
        <span class="n">dic_to_embed</span> <span class="o">=</span> <span class="n">to_gpu</span><span class="p">(</span><span class="n">args_cuda</span><span class="p">,</span> <span class="n">dic_to_embed</span><span class="p">)</span>
        <span class="n">embeded</span> <span class="o">=</span> <span class="n">autoencoder</span><span class="o">.</span><span class="n">embed_after_LSTM</span><span class="p">(</span><span class="n">dic_to_embed</span><span class="p">,</span> <span class="p">[</span><span class="n">n_dic_1</span><span class="p">,</span><span class="n">n_dic_1</span><span class="p">])</span>
        <span class="n">dic_vector</span> <span class="o">=</span> <span class="n">embeded</span><span class="p">[:</span><span class="n">n_dic_1</span><span class="p">]</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">ntokens</span><span class="o">/</span><span class="n">EMBED_SEGMENT</span><span class="p">):</span>
            <span class="n">dic_j</span> <span class="o">=</span> <span class="n">dic_i</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
            <span class="n">n_dic_j</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dic_j</span><span class="p">)</span>
            <span class="n">dic_j</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">((</span><span class="n">dic_j</span><span class="p">,</span><span class="n">dic_j</span><span class="p">))</span>
            <span class="n">dic_to_embed</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">dic_j</span><span class="p">))</span>
            <span class="n">dic_to_embed</span> <span class="o">=</span> <span class="n">to_gpu</span><span class="p">(</span><span class="n">args_cuda</span><span class="p">,</span> <span class="n">dic_to_embed</span><span class="p">)</span>
            <span class="n">embeded</span> <span class="o">=</span> <span class="n">autoencoder</span><span class="o">.</span><span class="n">embed_after_LSTM</span><span class="p">(</span><span class="n">dic_to_embed</span><span class="p">,</span> <span class="p">[</span><span class="n">n_dic_j</span><span class="p">,</span><span class="n">n_dic_j</span><span class="p">])</span>
            <span class="n">dic_vector_j</span> <span class="o">=</span> <span class="n">embeded</span><span class="p">[:</span><span class="n">n_dic_j</span><span class="p">]</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
            <span class="n">dic_vector</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">((</span><span class="n">dic_vector</span><span class="p">,</span> <span class="n">dic_vector_j</span><span class="p">))</span>
        <span class="n">dic</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">((</span><span class="n">dic</span><span class="p">,</span><span class="n">dic</span><span class="p">))</span>

    <span class="n">dic_tosave</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="n">dic_vector</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">dic</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>


    <span class="n">np</span><span class="o">.</span><span class="n">savetxt</span><span class="p">(</span><span class="s1">&#39;./output/{}/embed_afterLSTM_{}.txt&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">args_outf</span><span class="p">,</span> <span class="n">epoch</span><span class="p">),</span> <span class="n">dic_tosave</span><span class="p">,</span>
               <span class="n">fmt</span><span class="o">=</span><span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="s1">&#39;</span><span class="si">%i</span><span class="s1">&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="s1">&#39;</span><span class="si">%1.6f</span><span class="s1">&#39;</span><span class="p">]</span> <span class="o">*</span> <span class="n">emsize</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">dic_vector</span>

<span class="k">def</span> <span class="nf">embed_dic_for_loss</span><span class="p">(</span><span class="n">corpus</span><span class="p">,</span> <span class="n">emsize</span><span class="p">):</span>
    <span class="n">dic</span> <span class="o">=</span> <span class="n">corpus</span><span class="o">.</span><span class="n">dictionary</span><span class="o">.</span><span class="n">word2idx</span><span class="o">.</span><span class="n">values</span><span class="p">()</span>
    <span class="n">dic</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">dic</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">ntokens</span> <span class="o">&lt;=</span> <span class="n">EMBED_SEGMENT</span><span class="p">:</span>
        <span class="n">dic</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">((</span><span class="n">dic</span><span class="p">,</span><span class="n">dic</span><span class="p">))</span>
        <span class="n">dic_to_embed</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">dic</span><span class="p">))</span>
        <span class="n">dic_to_embed</span> <span class="o">=</span> <span class="n">to_gpu</span><span class="p">(</span><span class="n">args_cuda</span><span class="p">,</span> <span class="n">dic_to_embed</span><span class="p">)</span>
        <span class="n">embeded</span> <span class="o">=</span> <span class="n">autoencoder</span><span class="o">.</span><span class="n">embed_after_LSTM</span><span class="p">(</span><span class="n">dic_to_embed</span><span class="p">,</span> <span class="p">[</span><span class="n">ntokens</span><span class="p">,</span><span class="n">ntokens</span><span class="p">])</span>
        <span class="n">dic_vector</span> <span class="o">=</span> <span class="n">embeded</span><span class="p">[:</span><span class="n">ntokens</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">dic_i</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array_split</span><span class="p">(</span><span class="n">dic</span><span class="p">,</span> <span class="n">ntokens</span><span class="o">/</span><span class="n">EMBED_SEGMENT</span><span class="p">)</span>
        <span class="n">dic_1</span> <span class="o">=</span> <span class="n">dic_i</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">n_dic_1</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dic_1</span><span class="p">)</span>
        <span class="n">dic_1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">((</span><span class="n">dic_1</span><span class="p">,</span><span class="n">dic_1</span><span class="p">))</span>
        <span class="n">dic_to_embed</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">dic_1</span><span class="p">))</span>
        <span class="n">dic_to_embed</span> <span class="o">=</span> <span class="n">to_gpu</span><span class="p">(</span><span class="n">args_cuda</span><span class="p">,</span> <span class="n">dic_to_embed</span><span class="p">)</span>
        <span class="n">embeded</span> <span class="o">=</span> <span class="n">autoencoder</span><span class="o">.</span><span class="n">embed_after_LSTM</span><span class="p">(</span><span class="n">dic_to_embed</span><span class="p">,</span> <span class="p">[</span><span class="n">n_dic_1</span><span class="p">,</span><span class="n">n_dic_1</span><span class="p">])</span>
        <span class="n">dic_vector</span> <span class="o">=</span> <span class="n">embeded</span><span class="p">[:</span><span class="n">n_dic_1</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">ntokens</span><span class="o">/</span><span class="n">EMBED_SEGMENT</span><span class="p">):</span>
            <span class="n">dic_j</span> <span class="o">=</span> <span class="n">dic_i</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
            <span class="n">n_dic_j</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dic_j</span><span class="p">)</span>
            <span class="n">dic_j</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">((</span><span class="n">dic_j</span><span class="p">,</span><span class="n">dic_j</span><span class="p">))</span>
            <span class="n">dic_to_embed</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">dic_j</span><span class="p">))</span>
            <span class="n">dic_to_embed</span> <span class="o">=</span> <span class="n">to_gpu</span><span class="p">(</span><span class="n">args_cuda</span><span class="p">,</span> <span class="n">dic_to_embed</span><span class="p">)</span>
            <span class="n">embeded</span> <span class="o">=</span> <span class="n">autoencoder</span><span class="o">.</span><span class="n">embed_after_LSTM</span><span class="p">(</span><span class="n">dic_to_embed</span><span class="p">,</span> <span class="p">[</span><span class="n">n_dic_j</span><span class="p">,</span><span class="n">n_dic_j</span><span class="p">])</span>
            <span class="n">dic_vector_j</span> <span class="o">=</span> <span class="n">embeded</span><span class="p">[:</span><span class="n">n_dic_j</span><span class="p">]</span>
            <span class="n">dic_vector</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">dic_vector</span><span class="p">,</span> <span class="n">dic_vector_j</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">dic_vector</span>


<span class="k">def</span> <span class="nf">unique</span><span class="p">(</span><span class="n">tensor1d</span><span class="p">):</span>
    <span class="n">t</span><span class="p">,</span> <span class="n">idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">tensor1d</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">return_index</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">t</span><span class="p">,</span> <span class="n">idx</span>


<span class="k">def</span> <span class="nf">train_ae</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">total_loss_ae</span><span class="p">,</span> <span class="n">start_time</span><span class="p">,</span> <span class="n">i</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Training LSTM AE</span>
<span class="sd">    :param batch: one batch of data</span>
<span class="sd">    :param total_loss_ae: accumulated loss for LSTM AE so far</span>
<span class="sd">    :param start_time: for timming</span>
<span class="sd">    :param i: current iteration ID</span>
<span class="sd">    :return: accumulated total loss of LSTM AE part so far, and start time for timing</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">autoencoder</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="n">autoencoder</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

    <span class="n">source</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">lengths</span> <span class="o">=</span> <span class="n">batch</span>
    <span class="n">source</span> <span class="o">=</span> <span class="n">to_gpu</span><span class="p">(</span><span class="n">args_cuda</span><span class="p">,</span> <span class="n">Variable</span><span class="p">(</span><span class="n">source</span><span class="p">))</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">to_gpu</span><span class="p">(</span><span class="n">args_cuda</span><span class="p">,</span> <span class="n">Variable</span><span class="p">(</span><span class="n">target</span><span class="p">))</span>

    <span class="c1"># Create sentence length mask over padding</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">gt</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">masked_target</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">masked_select</span><span class="p">(</span><span class="n">mask</span><span class="p">)</span>
    <span class="c1"># examples x ntokens</span>
    <span class="n">output_mask</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">mask</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">ntokens</span><span class="p">)</span>

    <span class="c1"># output: batch x seq_len x ntokens</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">autoencoder</span><span class="p">(</span><span class="n">source</span><span class="p">,</span> <span class="n">lengths</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

    <span class="c1"># output_size: batch_size, maxlen, self.ntokens</span>
    <span class="n">flattened_output</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">ntokens</span><span class="p">)</span>


    <span class="n">emb_py</span> <span class="o">=</span> <span class="n">embed_dic_for_loss</span><span class="p">(</span><span class="n">corpus</span><span class="p">,</span> <span class="n">args_nhidden</span><span class="p">)</span>
    <span class="n">embT</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">emb_py</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">embT</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">embT</span><span class="p">,</span> <span class="n">L</span><span class="p">)</span>

    <span class="n">adj_loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">embT</span><span class="p">,</span> <span class="n">emb_py</span><span class="p">))</span> <span class="o">/</span> <span class="n">ntokens</span>

    <span class="n">masked_output</span> <span class="o">=</span> \
        <span class="n">flattened_output</span><span class="o">.</span><span class="n">masked_select</span><span class="p">(</span><span class="n">output_mask</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">ntokens</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion_ce</span><span class="p">(</span><span class="n">masked_output</span><span class="o">/</span><span class="n">args_temp</span><span class="p">,</span> <span class="n">masked_target</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s1">&#39;loss&#39;</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>

    <span class="n">loss</span> <span class="o">+=</span> <span class="n">adj_loss</span>

    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

    <span class="c1"># `clip_grad_norm` to prevent exploding gradient in RNNs / LSTMs</span>
    <span class="c1"># This is the version of Wasserstein GAN, which has gradient clipping</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_norm</span><span class="p">(</span><span class="n">autoencoder</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">args_clip</span><span class="p">)</span>
    <span class="n">optimizer_ae</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

    <span class="n">total_loss_ae</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">data</span>

    <span class="n">accuracy</span> <span class="o">=</span> <span class="bp">None</span>

    <span class="c1">######################## store log periodically ############################</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="n">args_log_interval</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">i</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="c1"># accuracy</span>
        <span class="n">probs</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">masked_output</span><span class="p">)</span>
        <span class="n">max_vals</span><span class="p">,</span> <span class="n">max_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">accuracy</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">max_indices</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="n">masked_target</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">())</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="n">cur_loss</span> <span class="o">=</span> <span class="n">total_loss_ae</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">args_log_interval</span>
        <span class="n">elapsed</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span>
        <span class="k">print</span><span class="p">(</span><span class="s1">&#39;| epoch {:3d} | {:5d}/{:5d} batches | ms/batch {:5.2f} | &#39;</span>
              <span class="s1">&#39;loss {:5.2f} | ppl {:8.2f} | acc {:8.2f}&#39;</span>
              <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_data</span><span class="p">),</span>
                      <span class="n">elapsed</span> <span class="o">*</span> <span class="mi">1000</span> <span class="o">/</span> <span class="n">args_log_interval</span><span class="p">,</span>
                      <span class="n">cur_loss</span><span class="p">,</span> <span class="n">math</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">cur_loss</span><span class="p">),</span> <span class="n">accuracy</span><span class="p">))</span>

        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&quot;./output/{}/logs.txt&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">args_outf</span><span class="p">),</span> <span class="s1">&#39;a&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s1">&#39;| epoch {:3d} | {:5d}/{:5d} batches | ms/batch {:5.2f} | &#39;</span>
                    <span class="s1">&#39;loss {:5.2f} | ppl {:8.2f} | acc {:8.2f}</span><span class="se">\n</span><span class="s1">&#39;</span><span class="o">.</span>
                    <span class="n">format</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_data</span><span class="p">),</span>
                           <span class="n">elapsed</span> <span class="o">*</span> <span class="mi">1000</span> <span class="o">/</span> <span class="n">args_log_interval</span><span class="p">,</span>
                           <span class="n">cur_loss</span><span class="p">,</span> <span class="n">math</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">cur_loss</span><span class="p">),</span> <span class="n">accuracy</span><span class="p">))</span>

        <span class="n">total_loss_ae</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">total_loss_ae</span><span class="p">,</span> <span class="n">start_time</span>


<span class="k">def</span> <span class="nf">train_gan_g</span><span class="p">():</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Training WGAN generator network</span>
<span class="sd">    :return: error of generator part</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">gan_gen</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="n">gan_gen</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

    <span class="n">noise</span> <span class="o">=</span> <span class="n">to_gpu</span><span class="p">(</span><span class="n">args_cuda</span><span class="p">,</span>
                   <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">args_batch_size</span><span class="p">,</span> <span class="n">args_z_size</span><span class="p">)))</span>
    <span class="n">noise</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="n">fake_hidden</span> <span class="o">=</span> <span class="n">gan_gen</span><span class="p">(</span><span class="n">noise</span><span class="p">)</span>
    <span class="n">errG</span> <span class="o">=</span> <span class="n">gan_disc</span><span class="p">(</span><span class="n">fake_hidden</span><span class="p">)</span>

    <span class="c1"># loss / backprop</span>
    <span class="n">errG</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">one</span><span class="p">)</span>
    <span class="n">optimizer_gan_g</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">errG</span>


<span class="k">def</span> <span class="nf">grad_hook</span><span class="p">(</span><span class="n">grad</span><span class="p">):</span>
    <span class="c1"># Gradient norm: regularize to be same</span>
    <span class="c1"># code_grad_gan * code_grad_ae / norm(code_grad_gan)</span>
    <span class="k">if</span> <span class="n">args_enc_grad_norm</span><span class="p">:</span>
        <span class="n">gan_norm</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
        <span class="n">normed_grad</span> <span class="o">=</span> <span class="n">grad</span> <span class="o">*</span> <span class="n">autoencoder</span><span class="o">.</span><span class="n">grad_norm</span> <span class="o">/</span> <span class="n">gan_norm</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">normed_grad</span> <span class="o">=</span> <span class="n">grad</span>

    <span class="c1"># weight factor and sign flip</span>
    <span class="n">normed_grad</span> <span class="o">*=</span> <span class="o">-</span><span class="n">math</span><span class="o">.</span><span class="n">fabs</span><span class="p">(</span><span class="n">args_gan_toenc</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">normed_grad</span>


<span class="k">def</span> <span class="nf">train_gan_d</span><span class="p">(</span><span class="n">batch</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Training WGAN discriminator</span>
<span class="sd">    :param batch: training batch data</span>
<span class="sd">    :return: discriminator part error</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># clamp parameters to a cube</span>
    <span class="c1"># WGAN Weight clipping</span>
    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">gan_disc</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
        <span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">clamp_</span><span class="p">(</span><span class="o">-</span><span class="n">args_gan_clamp</span><span class="p">,</span> <span class="n">args_gan_clamp</span><span class="p">)</span>

    <span class="n">autoencoder</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="n">autoencoder</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">gan_disc</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="n">gan_disc</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

    <span class="c1"># positive samples ----------------------------</span>
    <span class="c1"># generate real codes</span>
    <span class="n">source</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">lengths</span> <span class="o">=</span> <span class="n">batch</span>
    <span class="n">source</span> <span class="o">=</span> <span class="n">to_gpu</span><span class="p">(</span><span class="n">args_cuda</span><span class="p">,</span> <span class="n">Variable</span><span class="p">(</span><span class="n">source</span><span class="p">))</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">to_gpu</span><span class="p">(</span><span class="n">args_cuda</span><span class="p">,</span> <span class="n">Variable</span><span class="p">(</span><span class="n">target</span><span class="p">))</span>

    <span class="c1"># batch_size x nhidden</span>
    <span class="n">real_hidden</span> <span class="o">=</span> <span class="n">autoencoder</span><span class="p">(</span><span class="n">source</span><span class="p">,</span> <span class="n">lengths</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">encode_only</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">real_hidden</span><span class="o">.</span><span class="n">register_hook</span><span class="p">(</span><span class="n">grad_hook</span><span class="p">)</span>

    <span class="c1"># loss / backprop</span>
    <span class="n">errD_real</span> <span class="o">=</span> <span class="n">gan_disc</span><span class="p">(</span><span class="n">real_hidden</span><span class="p">)</span>
    <span class="n">errD_real</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">one</span><span class="p">)</span>

    <span class="c1"># negative samples ----------------------------</span>
    <span class="c1"># generate fake codes</span>
    <span class="n">noise</span> <span class="o">=</span> <span class="n">to_gpu</span><span class="p">(</span><span class="n">args_cuda</span><span class="p">,</span>
                   <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">args_batch_size</span><span class="p">,</span> <span class="n">args_z_size</span><span class="p">)))</span>
    <span class="n">noise</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="c1"># loss / backprop</span>
    <span class="n">fake_hidden</span> <span class="o">=</span> <span class="n">gan_gen</span><span class="p">(</span><span class="n">noise</span><span class="p">)</span>
    <span class="n">errD_fake</span> <span class="o">=</span> <span class="n">gan_disc</span><span class="p">(</span><span class="n">fake_hidden</span><span class="o">.</span><span class="n">detach</span><span class="p">())</span>
    <span class="n">errD_fake</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">mone</span><span class="p">)</span>

    <span class="c1"># `clip_grad_norm` to prvent exploding gradient problem in RNNs / LSTMs</span>
    <span class="c1"># This is the version of Wasserstein GAN, which has gradient clipping</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_norm</span><span class="p">(</span><span class="n">autoencoder</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">args_clip</span><span class="p">)</span>

    <span class="n">optimizer_gan_d</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="n">optimizer_ae</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="n">errD</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="n">errD_real</span> <span class="o">-</span> <span class="n">errD_fake</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">errD</span><span class="p">,</span> <span class="n">errD_real</span><span class="p">,</span> <span class="n">errD_fake</span>


<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Training...&quot;</span><span class="p">)</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&quot;./output/{}/logs.txt&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">args_outf</span><span class="p">),</span> <span class="s1">&#39;a&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s1">&#39;Training...</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="c1"># schedule of increasing GAN training loops</span>
<span class="k">if</span> <span class="n">args_niters_gan_schedule</span> <span class="o">!=</span> <span class="s2">&quot;&quot;</span><span class="p">:</span>
    <span class="n">gan_schedule</span> <span class="o">=</span> <span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">args_niters_gan_schedule</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;-&quot;</span><span class="p">)]</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">gan_schedule</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">niter_gan</span> <span class="o">=</span> <span class="mi">1</span>   <span class="c1"># start from 1, and will be dynamically increased</span>

<span class="n">fixed_noise</span> <span class="o">=</span> <span class="n">to_gpu</span><span class="p">(</span><span class="n">args_cuda</span><span class="p">,</span>
                     <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">args_batch_size</span><span class="p">,</span> <span class="n">args_z_size</span><span class="p">)))</span>
<span class="n">fixed_noise</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">one</span> <span class="o">=</span> <span class="n">to_gpu</span><span class="p">(</span><span class="n">args_cuda</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">([</span><span class="mi">1</span><span class="p">]))</span>
<span class="n">mone</span> <span class="o">=</span> <span class="n">one</span> <span class="o">*</span> <span class="o">-</span><span class="mi">1</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">args_epochs</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
    <span class="c1"># embed_afterLSTM(corpus, args_nhidden)</span>

    <span class="c1"># update gan training schedule</span>
    <span class="k">if</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="n">gan_schedule</span><span class="p">:</span>
        <span class="n">niter_gan</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">print</span><span class="p">(</span><span class="s2">&quot;GAN training loop schedule increased to {}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">niter_gan</span><span class="p">))</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&quot;./output/{}/logs.txt&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">args_outf</span><span class="p">),</span> <span class="s1">&#39;a&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s2">&quot;GAN training loop schedule increased to {}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span>
                    <span class="n">format</span><span class="p">(</span><span class="n">niter_gan</span><span class="p">))</span>

    <span class="n">total_loss_ae</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">epoch_start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">niter</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">niter_global</span> <span class="o">=</span> <span class="mi">1</span>

    <span class="c1"># loop through all batches in training data</span>
    <span class="k">while</span> <span class="n">niter</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_data</span><span class="p">):</span>

        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">            Iteratively conduct autoencoder training, then GAN regularization,</span>
<span class="sd">            The GAN part includes discriminator and generator iteratively.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># train autoencoder ----------------------------</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">args_niters_ae</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">niter</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_data</span><span class="p">):</span>
                <span class="k">break</span>  <span class="c1"># end of epoch</span>
            <span class="n">total_loss_ae</span><span class="p">,</span> <span class="n">start_time</span> <span class="o">=</span> \
                <span class="n">train_ae</span><span class="p">(</span><span class="n">train_data</span><span class="p">[</span><span class="n">niter</span><span class="p">],</span> <span class="n">total_loss_ae</span><span class="p">,</span> <span class="n">start_time</span><span class="p">,</span> <span class="n">niter</span><span class="p">)</span>
            <span class="n">niter</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="c1"># train gan ----------------------------------</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">niter_gan</span><span class="p">):</span>

            <span class="c1"># train discriminator/critic</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">args_niters_gan_d</span><span class="p">):</span>
                <span class="c1"># feed a seen sample within this epoch; good for early training</span>
                <span class="n">errD</span><span class="p">,</span> <span class="n">errD_real</span><span class="p">,</span> <span class="n">errD_fake</span> <span class="o">=</span> \
                    <span class="n">train_gan_d</span><span class="p">(</span><span class="n">train_data</span><span class="p">[</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_data</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">)])</span>

            <span class="c1"># train generator</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">args_niters_gan_g</span><span class="p">):</span>
                <span class="n">errG</span> <span class="o">=</span> <span class="n">train_gan_g</span><span class="p">()</span>



        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">            The codes here are for logging running status, not actually conduct the algorithm logic</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">niter_global</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="n">niter_global</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s1">&#39;[</span><span class="si">%d</span><span class="s1">/</span><span class="si">%d</span><span class="s1">][</span><span class="si">%d</span><span class="s1">/</span><span class="si">%d</span><span class="s1">] Loss_D: </span><span class="si">%.8f</span><span class="s1"> (Loss_D_real: </span><span class="si">%.8f</span><span class="s1"> &#39;</span>
                  <span class="s1">&#39;Loss_D_fake: </span><span class="si">%.8f</span><span class="s1">) Loss_G: </span><span class="si">%.8f</span><span class="s1">&#39;</span>
                  <span class="o">%</span> <span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">args_epochs</span><span class="p">,</span> <span class="n">niter</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_data</span><span class="p">),</span>
                     <span class="n">errD</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">errD_real</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                     <span class="n">errD_fake</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">errG</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
            <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&quot;./output/{}/logs.txt&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">args_outf</span><span class="p">),</span> <span class="s1">&#39;a&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
                <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s1">&#39;[</span><span class="si">%d</span><span class="s1">/</span><span class="si">%d</span><span class="s1">][</span><span class="si">%d</span><span class="s1">/</span><span class="si">%d</span><span class="s1">] Loss_D: </span><span class="si">%.8f</span><span class="s1"> (Loss_D_real: </span><span class="si">%.8f</span><span class="s1"> &#39;</span>
                        <span class="s1">&#39;Loss_D_fake: </span><span class="si">%.8f</span><span class="s1">) Loss_G: </span><span class="si">%.8f</span><span class="se">\n</span><span class="s1">&#39;</span>
                        <span class="o">%</span> <span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">args_epochs</span><span class="p">,</span> <span class="n">niter</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_data</span><span class="p">),</span>
                           <span class="n">errD</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">errD_real</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                           <span class="n">errD_fake</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">errG</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>

            <span class="c1"># exponentially decaying noise on autoencoder</span>
            <span class="n">autoencoder</span><span class="o">.</span><span class="n">noise_radius</span> <span class="o">=</span> \
                <span class="n">autoencoder</span><span class="o">.</span><span class="n">noise_radius</span><span class="o">*</span><span class="n">args_noise_anneal</span>


    <span class="c1"># embed_corpus(corpus, args_emsize)</span>
    <span class="n">embed_afterLSTM</span><span class="p">(</span><span class="n">corpus</span><span class="p">,</span> <span class="n">args_nhidden</span><span class="p">)</span>
    <span class="c1"># save_model()</span>

    <span class="c1"># shuffle between epochs</span>
    <span class="n">train_data</span> <span class="o">=</span> <span class="n">batchify</span><span class="p">(</span><span class="n">corpus</span><span class="o">.</span><span class="n">train</span><span class="p">,</span> <span class="n">args_batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1">###############################################################################</span>
<span class="c1"># visualization</span>
<span class="c1">###############################################################################</span>

<span class="c1"># input membership(label) of each node</span>
<span class="n">membership_path</span> <span class="o">=</span> <span class="s2">&quot;../tmp/membership.txt&quot;</span>

<span class="c1"># use last iteration output embeddings as code</span>
<span class="n">embedding_path</span> <span class="o">=</span> <span class="s2">&quot;./output/example/embed_afterLSTM_{}.txt&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">args_epochs</span><span class="p">)</span>

<span class="c1"># viz_tsne(membership_path, embedding_path)</span>
<span class="n">viz</span><span class="p">(</span><span class="n">membership_path</span><span class="p">,</span> <span class="n">embedding_path</span><span class="p">)</span>
</pre></div>

  </div>

  </header>

  <section id="section-items">
    <h2 class="section-title" id="header-variables">Module variables</h2>
      <div class="item">
      <p id="train2.A" class="name">var <span class="ident">A</span></p>
      
  
  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="train2.A_nx" class="name">var <span class="ident">A_nx</span></p>
      
  
  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="train2.EMBED_SEGMENT" class="name">var <span class="ident">EMBED_SEGMENT</span></p>
      
  
  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="train2.L" class="name">var <span class="ident">L</span></p>
      
  
  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="train2.args_arch_d" class="name">var <span class="ident">args_arch_d</span></p>
      
  
  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="train2.args_arch_g" class="name">var <span class="ident">args_arch_g</span></p>
      
  
  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="train2.args_batch_size" class="name">var <span class="ident">args_batch_size</span></p>
      
  
  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="train2.args_beta1" class="name">var <span class="ident">args_beta1</span></p>
      
  
  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="train2.args_clip" class="name">var <span class="ident">args_clip</span></p>
      
  
  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="train2.args_cuda" class="name">var <span class="ident">args_cuda</span></p>
      
  
  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="train2.args_data_path" class="name">var <span class="ident">args_data_path</span></p>
      
  
  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="train2.args_dropout" class="name">var <span class="ident">args_dropout</span></p>
      
  
  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="train2.args_emsize" class="name">var <span class="ident">args_emsize</span></p>
      
  
  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="train2.args_enc_grad_norm" class="name">var <span class="ident">args_enc_grad_norm</span></p>
      
  
  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="train2.args_epochs" class="name">var <span class="ident">args_epochs</span></p>
      
  
  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="train2.args_gan_clamp" class="name">var <span class="ident">args_gan_clamp</span></p>
      
  
  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="train2.args_gan_toenc" class="name">var <span class="ident">args_gan_toenc</span></p>
      
  
  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="train2.args_hidden_init" class="name">var <span class="ident">args_hidden_init</span></p>
      
  
  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="train2.args_log_interval" class="name">var <span class="ident">args_log_interval</span></p>
      
  
  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="train2.args_lr_ae" class="name">var <span class="ident">args_lr_ae</span></p>
      
  
  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="train2.args_lr_gan_d" class="name">var <span class="ident">args_lr_gan_d</span></p>
      
  
  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="train2.args_lr_gan_g" class="name">var <span class="ident">args_lr_gan_g</span></p>
      
  
  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="train2.args_maxlen" class="name">var <span class="ident">args_maxlen</span></p>
      
  
  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="train2.args_min_epochs" class="name">var <span class="ident">args_min_epochs</span></p>
      
  
  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="train2.args_nhidden" class="name">var <span class="ident">args_nhidden</span></p>
      
  
  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="train2.args_niters_ae" class="name">var <span class="ident">args_niters_ae</span></p>
      
  
  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="train2.args_niters_gan_d" class="name">var <span class="ident">args_niters_gan_d</span></p>
      
  
  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="train2.args_niters_gan_g" class="name">var <span class="ident">args_niters_gan_g</span></p>
      
  
  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="train2.args_niters_gan_schedule" class="name">var <span class="ident">args_niters_gan_schedule</span></p>
      
  
  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="train2.args_nlayers" class="name">var <span class="ident">args_nlayers</span></p>
      
  
  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="train2.args_no_earlystopping" class="name">var <span class="ident">args_no_earlystopping</span></p>
      
  
  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="train2.args_noise_anneal" class="name">var <span class="ident">args_noise_anneal</span></p>
      
  
  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="train2.args_noise_radius" class="name">var <span class="ident">args_noise_radius</span></p>
      
  
  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="train2.args_ntokens" class="name">var <span class="ident">args_ntokens</span></p>
      
  
  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="train2.args_numWalks_per_node" class="name">var <span class="ident">args_numWalks_per_node</span></p>
      
  
  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="train2.args_outf" class="name">var <span class="ident">args_outf</span></p>
      
  
  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="train2.args_sample" class="name">var <span class="ident">args_sample</span></p>
      
  
  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="train2.args_seed" class="name">var <span class="ident">args_seed</span></p>
      
  
  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="train2.args_temp" class="name">var <span class="ident">args_temp</span></p>
      
  
  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="train2.args_walk_length" class="name">var <span class="ident">args_walk_length</span></p>
      
  
  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="train2.args_z_size" class="name">var <span class="ident">args_z_size</span></p>
      
  
  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="train2.autoencoder" class="name">var <span class="ident">autoencoder</span></p>
      
  
  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="train2.corpus" class="name">var <span class="ident">corpus</span></p>
      
  
  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="train2.criterion_ce" class="name">var <span class="ident">criterion_ce</span></p>
      
  
  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="train2.embedding_path" class="name">var <span class="ident">embedding_path</span></p>
      
  
  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="train2.epoch" class="name">var <span class="ident">epoch</span></p>
      
  
  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="train2.epoch_start_time" class="name">var <span class="ident">epoch_start_time</span></p>
      
  
  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="train2.f" class="name">var <span class="ident">f</span></p>
      
  
  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="train2.fixed_noise" class="name">var <span class="ident">fixed_noise</span></p>
      
  
  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="train2.gan_disc" class="name">var <span class="ident">gan_disc</span></p>
      
  
  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="train2.gan_gen" class="name">var <span class="ident">gan_gen</span></p>
      
  
  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="train2.gan_schedule" class="name">var <span class="ident">gan_schedule</span></p>
      
  
  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="train2.i" class="name">var <span class="ident">i</span></p>
      
  
  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="train2.k" class="name">var <span class="ident">k</span></p>
      
  
  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="train2.membership_path" class="name">var <span class="ident">membership_path</span></p>
      
  
  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="train2.mone" class="name">var <span class="ident">mone</span></p>
      
  
  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="train2.niter" class="name">var <span class="ident">niter</span></p>
      
  
  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="train2.niter_gan" class="name">var <span class="ident">niter_gan</span></p>
      
  
  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="train2.niter_global" class="name">var <span class="ident">niter_global</span></p>
      
  
  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="train2.ntokens" class="name">var <span class="ident">ntokens</span></p>
      
  
  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="train2.one" class="name">var <span class="ident">one</span></p>
      
  
  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="train2.optimizer_ae" class="name">var <span class="ident">optimizer_ae</span></p>
      
  
  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="train2.optimizer_gan_d" class="name">var <span class="ident">optimizer_gan_d</span></p>
      
  
  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="train2.optimizer_gan_g" class="name">var <span class="ident">optimizer_gan_g</span></p>
      
  
  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="train2.start_time" class="name">var <span class="ident">start_time</span></p>
      
  
  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="train2.train_data" class="name">var <span class="ident">train_data</span></p>
      
  
  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="train2.walks" class="name">var <span class="ident">walks</span></p>
      
  
  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="train2.x" class="name">var <span class="ident">x</span></p>
      
  
  <div class="source_cont">
</div>

      </div>

    <h2 class="section-title" id="header-functions">Functions</h2>
      
  <div class="item">
    <div class="name def" id="train2.embed_afterLSTM">
    <p>def <span class="ident">embed_afterLSTM</span>(</p><p>corpus, emsize)</p>
    </div>
    

    
  
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-train2.embed_afterLSTM', this);">Show source &equiv;</a></p>
  <div id="source-train2.embed_afterLSTM" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">embed_afterLSTM</span><span class="p">(</span><span class="n">corpus</span><span class="p">,</span> <span class="n">emsize</span><span class="p">):</span>
    <span class="n">dic</span> <span class="o">=</span> <span class="n">corpus</span><span class="o">.</span><span class="n">dictionary</span><span class="o">.</span><span class="n">word2idx</span><span class="o">.</span><span class="n">values</span><span class="p">()</span>
    <span class="n">dic</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">dic</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">ntokens</span> <span class="o">&lt;=</span> <span class="n">EMBED_SEGMENT</span><span class="p">:</span>
        <span class="n">dic</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">((</span><span class="n">dic</span><span class="p">,</span><span class="n">dic</span><span class="p">))</span>
        <span class="n">dic_to_embed</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">dic</span><span class="p">))</span>
        <span class="n">dic_to_embed</span> <span class="o">=</span> <span class="n">to_gpu</span><span class="p">(</span><span class="n">args_cuda</span><span class="p">,</span> <span class="n">dic_to_embed</span><span class="p">)</span>
        <span class="n">embeded</span> <span class="o">=</span> <span class="n">autoencoder</span><span class="o">.</span><span class="n">embed_after_LSTM</span><span class="p">(</span><span class="n">dic_to_embed</span><span class="p">,</span> <span class="p">[</span><span class="n">ntokens</span><span class="p">,</span><span class="n">ntokens</span><span class="p">])</span>
        <span class="n">dic_vector</span> <span class="o">=</span> <span class="n">embeded</span><span class="p">[:</span><span class="n">ntokens</span><span class="p">]</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">dic_i</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array_split</span><span class="p">(</span><span class="n">dic</span><span class="p">,</span> <span class="n">ntokens</span><span class="o">/</span><span class="n">EMBED_SEGMENT</span><span class="p">)</span>
        <span class="n">dic_1</span> <span class="o">=</span> <span class="n">dic_i</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">n_dic_1</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dic_1</span><span class="p">)</span>
        <span class="n">dic_1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">((</span><span class="n">dic_1</span><span class="p">,</span><span class="n">dic_1</span><span class="p">))</span>
        <span class="n">dic_to_embed</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">dic_1</span><span class="p">))</span>
        <span class="n">dic_to_embed</span> <span class="o">=</span> <span class="n">to_gpu</span><span class="p">(</span><span class="n">args_cuda</span><span class="p">,</span> <span class="n">dic_to_embed</span><span class="p">)</span>
        <span class="n">embeded</span> <span class="o">=</span> <span class="n">autoencoder</span><span class="o">.</span><span class="n">embed_after_LSTM</span><span class="p">(</span><span class="n">dic_to_embed</span><span class="p">,</span> <span class="p">[</span><span class="n">n_dic_1</span><span class="p">,</span><span class="n">n_dic_1</span><span class="p">])</span>
        <span class="n">dic_vector</span> <span class="o">=</span> <span class="n">embeded</span><span class="p">[:</span><span class="n">n_dic_1</span><span class="p">]</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">ntokens</span><span class="o">/</span><span class="n">EMBED_SEGMENT</span><span class="p">):</span>
            <span class="n">dic_j</span> <span class="o">=</span> <span class="n">dic_i</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
            <span class="n">n_dic_j</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dic_j</span><span class="p">)</span>
            <span class="n">dic_j</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">((</span><span class="n">dic_j</span><span class="p">,</span><span class="n">dic_j</span><span class="p">))</span>
            <span class="n">dic_to_embed</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">dic_j</span><span class="p">))</span>
            <span class="n">dic_to_embed</span> <span class="o">=</span> <span class="n">to_gpu</span><span class="p">(</span><span class="n">args_cuda</span><span class="p">,</span> <span class="n">dic_to_embed</span><span class="p">)</span>
            <span class="n">embeded</span> <span class="o">=</span> <span class="n">autoencoder</span><span class="o">.</span><span class="n">embed_after_LSTM</span><span class="p">(</span><span class="n">dic_to_embed</span><span class="p">,</span> <span class="p">[</span><span class="n">n_dic_j</span><span class="p">,</span><span class="n">n_dic_j</span><span class="p">])</span>
            <span class="n">dic_vector_j</span> <span class="o">=</span> <span class="n">embeded</span><span class="p">[:</span><span class="n">n_dic_j</span><span class="p">]</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
            <span class="n">dic_vector</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">((</span><span class="n">dic_vector</span><span class="p">,</span> <span class="n">dic_vector_j</span><span class="p">))</span>
        <span class="n">dic</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">((</span><span class="n">dic</span><span class="p">,</span><span class="n">dic</span><span class="p">))</span>

    <span class="n">dic_tosave</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="n">dic_vector</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">dic</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>


    <span class="n">np</span><span class="o">.</span><span class="n">savetxt</span><span class="p">(</span><span class="s1">&#39;./output/{}/embed_afterLSTM_{}.txt&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">args_outf</span><span class="p">,</span> <span class="n">epoch</span><span class="p">),</span> <span class="n">dic_tosave</span><span class="p">,</span>
               <span class="n">fmt</span><span class="o">=</span><span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="s1">&#39;</span><span class="si">%i</span><span class="s1">&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="s1">&#39;</span><span class="si">%1.6f</span><span class="s1">&#39;</span><span class="p">]</span> <span class="o">*</span> <span class="n">emsize</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">dic_vector</span>
</pre></div>

  </div>
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="train2.embed_dic_for_loss">
    <p>def <span class="ident">embed_dic_for_loss</span>(</p><p>corpus, emsize)</p>
    </div>
    

    
  
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-train2.embed_dic_for_loss', this);">Show source &equiv;</a></p>
  <div id="source-train2.embed_dic_for_loss" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">embed_dic_for_loss</span><span class="p">(</span><span class="n">corpus</span><span class="p">,</span> <span class="n">emsize</span><span class="p">):</span>
    <span class="n">dic</span> <span class="o">=</span> <span class="n">corpus</span><span class="o">.</span><span class="n">dictionary</span><span class="o">.</span><span class="n">word2idx</span><span class="o">.</span><span class="n">values</span><span class="p">()</span>
    <span class="n">dic</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">dic</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">ntokens</span> <span class="o">&lt;=</span> <span class="n">EMBED_SEGMENT</span><span class="p">:</span>
        <span class="n">dic</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">((</span><span class="n">dic</span><span class="p">,</span><span class="n">dic</span><span class="p">))</span>
        <span class="n">dic_to_embed</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">dic</span><span class="p">))</span>
        <span class="n">dic_to_embed</span> <span class="o">=</span> <span class="n">to_gpu</span><span class="p">(</span><span class="n">args_cuda</span><span class="p">,</span> <span class="n">dic_to_embed</span><span class="p">)</span>
        <span class="n">embeded</span> <span class="o">=</span> <span class="n">autoencoder</span><span class="o">.</span><span class="n">embed_after_LSTM</span><span class="p">(</span><span class="n">dic_to_embed</span><span class="p">,</span> <span class="p">[</span><span class="n">ntokens</span><span class="p">,</span><span class="n">ntokens</span><span class="p">])</span>
        <span class="n">dic_vector</span> <span class="o">=</span> <span class="n">embeded</span><span class="p">[:</span><span class="n">ntokens</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">dic_i</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array_split</span><span class="p">(</span><span class="n">dic</span><span class="p">,</span> <span class="n">ntokens</span><span class="o">/</span><span class="n">EMBED_SEGMENT</span><span class="p">)</span>
        <span class="n">dic_1</span> <span class="o">=</span> <span class="n">dic_i</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">n_dic_1</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dic_1</span><span class="p">)</span>
        <span class="n">dic_1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">((</span><span class="n">dic_1</span><span class="p">,</span><span class="n">dic_1</span><span class="p">))</span>
        <span class="n">dic_to_embed</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">dic_1</span><span class="p">))</span>
        <span class="n">dic_to_embed</span> <span class="o">=</span> <span class="n">to_gpu</span><span class="p">(</span><span class="n">args_cuda</span><span class="p">,</span> <span class="n">dic_to_embed</span><span class="p">)</span>
        <span class="n">embeded</span> <span class="o">=</span> <span class="n">autoencoder</span><span class="o">.</span><span class="n">embed_after_LSTM</span><span class="p">(</span><span class="n">dic_to_embed</span><span class="p">,</span> <span class="p">[</span><span class="n">n_dic_1</span><span class="p">,</span><span class="n">n_dic_1</span><span class="p">])</span>
        <span class="n">dic_vector</span> <span class="o">=</span> <span class="n">embeded</span><span class="p">[:</span><span class="n">n_dic_1</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">ntokens</span><span class="o">/</span><span class="n">EMBED_SEGMENT</span><span class="p">):</span>
            <span class="n">dic_j</span> <span class="o">=</span> <span class="n">dic_i</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
            <span class="n">n_dic_j</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dic_j</span><span class="p">)</span>
            <span class="n">dic_j</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">((</span><span class="n">dic_j</span><span class="p">,</span><span class="n">dic_j</span><span class="p">))</span>
            <span class="n">dic_to_embed</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">dic_j</span><span class="p">))</span>
            <span class="n">dic_to_embed</span> <span class="o">=</span> <span class="n">to_gpu</span><span class="p">(</span><span class="n">args_cuda</span><span class="p">,</span> <span class="n">dic_to_embed</span><span class="p">)</span>
            <span class="n">embeded</span> <span class="o">=</span> <span class="n">autoencoder</span><span class="o">.</span><span class="n">embed_after_LSTM</span><span class="p">(</span><span class="n">dic_to_embed</span><span class="p">,</span> <span class="p">[</span><span class="n">n_dic_j</span><span class="p">,</span><span class="n">n_dic_j</span><span class="p">])</span>
            <span class="n">dic_vector_j</span> <span class="o">=</span> <span class="n">embeded</span><span class="p">[:</span><span class="n">n_dic_j</span><span class="p">]</span>
            <span class="n">dic_vector</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">dic_vector</span><span class="p">,</span> <span class="n">dic_vector_j</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">dic_vector</span>
</pre></div>

  </div>
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="train2.grad_hook">
    <p>def <span class="ident">grad_hook</span>(</p><p>grad)</p>
    </div>
    

    
  
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-train2.grad_hook', this);">Show source &equiv;</a></p>
  <div id="source-train2.grad_hook" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">grad_hook</span><span class="p">(</span><span class="n">grad</span><span class="p">):</span>
    <span class="c1"># Gradient norm: regularize to be same</span>
    <span class="c1"># code_grad_gan * code_grad_ae / norm(code_grad_gan)</span>
    <span class="k">if</span> <span class="n">args_enc_grad_norm</span><span class="p">:</span>
        <span class="n">gan_norm</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
        <span class="n">normed_grad</span> <span class="o">=</span> <span class="n">grad</span> <span class="o">*</span> <span class="n">autoencoder</span><span class="o">.</span><span class="n">grad_norm</span> <span class="o">/</span> <span class="n">gan_norm</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">normed_grad</span> <span class="o">=</span> <span class="n">grad</span>

    <span class="c1"># weight factor and sign flip</span>
    <span class="n">normed_grad</span> <span class="o">*=</span> <span class="o">-</span><span class="n">math</span><span class="o">.</span><span class="n">fabs</span><span class="p">(</span><span class="n">args_gan_toenc</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">normed_grad</span>
</pre></div>

  </div>
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="train2.save_model">
    <p>def <span class="ident">save_model</span>(</p><p>)</p>
    </div>
    

    
  
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-train2.save_model', this);">Show source &equiv;</a></p>
  <div id="source-train2.save_model" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">save_model</span><span class="p">():</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">&quot;Saving models&quot;</span><span class="p">)</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;./output/{}/autoencoder_model.pt&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">args_outf</span><span class="p">),</span> <span class="s1">&#39;wb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">autoencoder</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">f</span><span class="p">)</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;./output/{}/gan_gen_model.pt&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">args_outf</span><span class="p">),</span> <span class="s1">&#39;wb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">gan_gen</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">f</span><span class="p">)</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;./output/{}/gan_disc_model.pt&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">args_outf</span><span class="p">),</span> <span class="s1">&#39;wb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">gan_disc</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">f</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="train2.train_ae">
    <p>def <span class="ident">train_ae</span>(</p><p>batch, total_loss_ae, start_time, i)</p>
    </div>
    

    
  
    <div class="desc"><p>Training LSTM AE
:param batch: one batch of data
:param total_loss_ae: accumulated loss for LSTM AE so far
:param start_time: for timming
:param i: current iteration ID
:return: accumulated total loss of LSTM AE part so far, and start time for timing</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-train2.train_ae', this);">Show source &equiv;</a></p>
  <div id="source-train2.train_ae" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">train_ae</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">total_loss_ae</span><span class="p">,</span> <span class="n">start_time</span><span class="p">,</span> <span class="n">i</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Training LSTM AE</span>
<span class="sd">    :param batch: one batch of data</span>
<span class="sd">    :param total_loss_ae: accumulated loss for LSTM AE so far</span>
<span class="sd">    :param start_time: for timming</span>
<span class="sd">    :param i: current iteration ID</span>
<span class="sd">    :return: accumulated total loss of LSTM AE part so far, and start time for timing</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">autoencoder</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="n">autoencoder</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

    <span class="n">source</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">lengths</span> <span class="o">=</span> <span class="n">batch</span>
    <span class="n">source</span> <span class="o">=</span> <span class="n">to_gpu</span><span class="p">(</span><span class="n">args_cuda</span><span class="p">,</span> <span class="n">Variable</span><span class="p">(</span><span class="n">source</span><span class="p">))</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">to_gpu</span><span class="p">(</span><span class="n">args_cuda</span><span class="p">,</span> <span class="n">Variable</span><span class="p">(</span><span class="n">target</span><span class="p">))</span>

    <span class="c1"># Create sentence length mask over padding</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">gt</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">masked_target</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">masked_select</span><span class="p">(</span><span class="n">mask</span><span class="p">)</span>
    <span class="c1"># examples x ntokens</span>
    <span class="n">output_mask</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">mask</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">ntokens</span><span class="p">)</span>

    <span class="c1"># output: batch x seq_len x ntokens</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">autoencoder</span><span class="p">(</span><span class="n">source</span><span class="p">,</span> <span class="n">lengths</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

    <span class="c1"># output_size: batch_size, maxlen, self.ntokens</span>
    <span class="n">flattened_output</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">ntokens</span><span class="p">)</span>


    <span class="n">emb_py</span> <span class="o">=</span> <span class="n">embed_dic_for_loss</span><span class="p">(</span><span class="n">corpus</span><span class="p">,</span> <span class="n">args_nhidden</span><span class="p">)</span>
    <span class="n">embT</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">emb_py</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">embT</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">embT</span><span class="p">,</span> <span class="n">L</span><span class="p">)</span>

    <span class="n">adj_loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">embT</span><span class="p">,</span> <span class="n">emb_py</span><span class="p">))</span> <span class="o">/</span> <span class="n">ntokens</span>

    <span class="n">masked_output</span> <span class="o">=</span> \
        <span class="n">flattened_output</span><span class="o">.</span><span class="n">masked_select</span><span class="p">(</span><span class="n">output_mask</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">ntokens</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion_ce</span><span class="p">(</span><span class="n">masked_output</span><span class="o">/</span><span class="n">args_temp</span><span class="p">,</span> <span class="n">masked_target</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s1">&#39;loss&#39;</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>

    <span class="n">loss</span> <span class="o">+=</span> <span class="n">adj_loss</span>

    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

    <span class="c1"># `clip_grad_norm` to prevent exploding gradient in RNNs / LSTMs</span>
    <span class="c1"># This is the version of Wasserstein GAN, which has gradient clipping</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_norm</span><span class="p">(</span><span class="n">autoencoder</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">args_clip</span><span class="p">)</span>
    <span class="n">optimizer_ae</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

    <span class="n">total_loss_ae</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">data</span>

    <span class="n">accuracy</span> <span class="o">=</span> <span class="bp">None</span>

    <span class="c1">######################## store log periodically ############################</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="n">args_log_interval</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">i</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="c1"># accuracy</span>
        <span class="n">probs</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">masked_output</span><span class="p">)</span>
        <span class="n">max_vals</span><span class="p">,</span> <span class="n">max_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">accuracy</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">max_indices</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="n">masked_target</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">())</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="n">cur_loss</span> <span class="o">=</span> <span class="n">total_loss_ae</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">args_log_interval</span>
        <span class="n">elapsed</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span>
        <span class="k">print</span><span class="p">(</span><span class="s1">&#39;| epoch {:3d} | {:5d}/{:5d} batches | ms/batch {:5.2f} | &#39;</span>
              <span class="s1">&#39;loss {:5.2f} | ppl {:8.2f} | acc {:8.2f}&#39;</span>
              <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_data</span><span class="p">),</span>
                      <span class="n">elapsed</span> <span class="o">*</span> <span class="mi">1000</span> <span class="o">/</span> <span class="n">args_log_interval</span><span class="p">,</span>
                      <span class="n">cur_loss</span><span class="p">,</span> <span class="n">math</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">cur_loss</span><span class="p">),</span> <span class="n">accuracy</span><span class="p">))</span>

        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&quot;./output/{}/logs.txt&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">args_outf</span><span class="p">),</span> <span class="s1">&#39;a&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s1">&#39;| epoch {:3d} | {:5d}/{:5d} batches | ms/batch {:5.2f} | &#39;</span>
                    <span class="s1">&#39;loss {:5.2f} | ppl {:8.2f} | acc {:8.2f}</span><span class="se">\n</span><span class="s1">&#39;</span><span class="o">.</span>
                    <span class="n">format</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_data</span><span class="p">),</span>
                           <span class="n">elapsed</span> <span class="o">*</span> <span class="mi">1000</span> <span class="o">/</span> <span class="n">args_log_interval</span><span class="p">,</span>
                           <span class="n">cur_loss</span><span class="p">,</span> <span class="n">math</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">cur_loss</span><span class="p">),</span> <span class="n">accuracy</span><span class="p">))</span>

        <span class="n">total_loss_ae</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">total_loss_ae</span><span class="p">,</span> <span class="n">start_time</span>
</pre></div>

  </div>
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="train2.train_gan_d">
    <p>def <span class="ident">train_gan_d</span>(</p><p>batch)</p>
    </div>
    

    
  
    <div class="desc"><p>Training WGAN discriminator
:param batch: training batch data
:return: discriminator part error</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-train2.train_gan_d', this);">Show source &equiv;</a></p>
  <div id="source-train2.train_gan_d" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">train_gan_d</span><span class="p">(</span><span class="n">batch</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Training WGAN discriminator</span>
<span class="sd">    :param batch: training batch data</span>
<span class="sd">    :return: discriminator part error</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># clamp parameters to a cube</span>
    <span class="c1"># WGAN Weight clipping</span>
    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">gan_disc</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
        <span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">clamp_</span><span class="p">(</span><span class="o">-</span><span class="n">args_gan_clamp</span><span class="p">,</span> <span class="n">args_gan_clamp</span><span class="p">)</span>

    <span class="n">autoencoder</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="n">autoencoder</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">gan_disc</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="n">gan_disc</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

    <span class="c1"># positive samples ----------------------------</span>
    <span class="c1"># generate real codes</span>
    <span class="n">source</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">lengths</span> <span class="o">=</span> <span class="n">batch</span>
    <span class="n">source</span> <span class="o">=</span> <span class="n">to_gpu</span><span class="p">(</span><span class="n">args_cuda</span><span class="p">,</span> <span class="n">Variable</span><span class="p">(</span><span class="n">source</span><span class="p">))</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">to_gpu</span><span class="p">(</span><span class="n">args_cuda</span><span class="p">,</span> <span class="n">Variable</span><span class="p">(</span><span class="n">target</span><span class="p">))</span>

    <span class="c1"># batch_size x nhidden</span>
    <span class="n">real_hidden</span> <span class="o">=</span> <span class="n">autoencoder</span><span class="p">(</span><span class="n">source</span><span class="p">,</span> <span class="n">lengths</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">encode_only</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">real_hidden</span><span class="o">.</span><span class="n">register_hook</span><span class="p">(</span><span class="n">grad_hook</span><span class="p">)</span>

    <span class="c1"># loss / backprop</span>
    <span class="n">errD_real</span> <span class="o">=</span> <span class="n">gan_disc</span><span class="p">(</span><span class="n">real_hidden</span><span class="p">)</span>
    <span class="n">errD_real</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">one</span><span class="p">)</span>

    <span class="c1"># negative samples ----------------------------</span>
    <span class="c1"># generate fake codes</span>
    <span class="n">noise</span> <span class="o">=</span> <span class="n">to_gpu</span><span class="p">(</span><span class="n">args_cuda</span><span class="p">,</span>
                   <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">args_batch_size</span><span class="p">,</span> <span class="n">args_z_size</span><span class="p">)))</span>
    <span class="n">noise</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="c1"># loss / backprop</span>
    <span class="n">fake_hidden</span> <span class="o">=</span> <span class="n">gan_gen</span><span class="p">(</span><span class="n">noise</span><span class="p">)</span>
    <span class="n">errD_fake</span> <span class="o">=</span> <span class="n">gan_disc</span><span class="p">(</span><span class="n">fake_hidden</span><span class="o">.</span><span class="n">detach</span><span class="p">())</span>
    <span class="n">errD_fake</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">mone</span><span class="p">)</span>

    <span class="c1"># `clip_grad_norm` to prvent exploding gradient problem in RNNs / LSTMs</span>
    <span class="c1"># This is the version of Wasserstein GAN, which has gradient clipping</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_norm</span><span class="p">(</span><span class="n">autoencoder</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">args_clip</span><span class="p">)</span>

    <span class="n">optimizer_gan_d</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="n">optimizer_ae</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="n">errD</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="n">errD_real</span> <span class="o">-</span> <span class="n">errD_fake</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">errD</span><span class="p">,</span> <span class="n">errD_real</span><span class="p">,</span> <span class="n">errD_fake</span>
</pre></div>

  </div>
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="train2.train_gan_g">
    <p>def <span class="ident">train_gan_g</span>(</p><p>)</p>
    </div>
    

    
  
    <div class="desc"><p>Training WGAN generator network
:return: error of generator part</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-train2.train_gan_g', this);">Show source &equiv;</a></p>
  <div id="source-train2.train_gan_g" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">train_gan_g</span><span class="p">():</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Training WGAN generator network</span>
<span class="sd">    :return: error of generator part</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">gan_gen</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="n">gan_gen</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

    <span class="n">noise</span> <span class="o">=</span> <span class="n">to_gpu</span><span class="p">(</span><span class="n">args_cuda</span><span class="p">,</span>
                   <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">args_batch_size</span><span class="p">,</span> <span class="n">args_z_size</span><span class="p">)))</span>
    <span class="n">noise</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="n">fake_hidden</span> <span class="o">=</span> <span class="n">gan_gen</span><span class="p">(</span><span class="n">noise</span><span class="p">)</span>
    <span class="n">errG</span> <span class="o">=</span> <span class="n">gan_disc</span><span class="p">(</span><span class="n">fake_hidden</span><span class="p">)</span>

    <span class="c1"># loss / backprop</span>
    <span class="n">errG</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">one</span><span class="p">)</span>
    <span class="n">optimizer_gan_g</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">errG</span>
</pre></div>

  </div>
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="train2.unique">
    <p>def <span class="ident">unique</span>(</p><p>tensor1d)</p>
    </div>
    

    
  
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-train2.unique', this);">Show source &equiv;</a></p>
  <div id="source-train2.unique" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">unique</span><span class="p">(</span><span class="n">tensor1d</span><span class="p">):</span>
    <span class="n">t</span><span class="p">,</span> <span class="n">idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">tensor1d</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">return_index</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">t</span><span class="p">,</span> <span class="n">idx</span>
</pre></div>

  </div>
</div>

  </div>
  


  </section>

    </article>
  <div class="clear"> </div>
  <footer id="footer">
    <p>
      Documentation generated by
      <a href="https://github.com/BurntSushi/pdoc">pdoc 0.3.2</a>
    </p>

    <p>pdoc is in the public domain with the
      <a href="http://unlicense.org">UNLICENSE</a></p>

    <p>Design by <a href="http://nadh.in">Kailash Nadh</a></p>
  </footer>
</div>
</body>
</html>
